{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU" },"cells":[{"cell_type":"code","source":["!pip install transformers\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CL1vBdsKIHR0","outputId":"ee5e901c-d6bc-402f-e47c-4953c92642ac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.36.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.2.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7F0uNrW5ZK47","executionInfo":{"status":"ok","timestamp":1748091871878,"user_tz":-120,"elapsed":26633,"user":{"displayName":"Krittika Sardar","userId":"09484786934522619402"}},"outputId":"9f6b41a9-1add-4e3c-9c0e-49a8eda9b60b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install numpy --upgrade"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xKvtH_NRIj9t","outputId":"e50ebeb9-4722-4609-d285-08bb2e5156a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.24.4)\n","Collecting numpy\n","  Using cached numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n","Using cached numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n","Installing collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.24.4\n","    Uninstalling numpy-1.24.4:\n","      Successfully uninstalled numpy-1.24.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n","sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.2 which is incompatible.\n","numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n","tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-2.2.6\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Upload train.csv and test.csv\n","# from google.colab import files\n","# uploaded = files.upload()\n","\n","train_df = pd.read_csv(\"/content/drive/MyDrive/Web_Mining_DistilBert/balanced_train_data_updated.csv\")\n","test_df = pd.read_csv(\"/content/drive/MyDrive/Web_Mining_DistilBert/balanced_test_data_updated.csv\")\n","\n","# Combine columns if needed\n","train_df['text'] = train_df['Summary'].fillna('') + \" \" + train_df['Text'].fillna('')\n","test_df['text'] = test_df['Summary'].fillna('') + \" \" + test_df['Text'].fillna('')\n","\n","# Map labels to integers if not already encoded\n","train_df['label'] = train_df['Score']\n","test_df['label'] = test_df['Score']"],"metadata":{"id":"ImT3CgHPIIXE","executionInfo":{"status":"ok","timestamp":1748091941146,"user_tz":-120,"elapsed":5166,"user":{"displayName":"Krittika Sardar","userId":"09484786934522619402"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from transformers import DistilBertTokenizer\n","\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","\n","def encode(texts, labels, max_len=128):\n","    encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_len, return_tensors='pt')\n","    return encodings['input_ids'], encodings['attention_mask'], torch.tensor(labels)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["3f67e31338b44c0f92c5b6d780cce52b","92633ae4a3cb4d96a3da9957e5ae4576","b5003bee089a40ab9388191e20e8728b","1dfcb97d1bce4404a9345c131c324690","19b9936ab57d42979794b80220398c4e","279d61cc2c574c768d9369b2357ef671","00eca0f4ef174781b3ece9b24cbd1b0f","92a0cd53ff2f4dcebf190a10ad3b6256","05ad0165bf744953a682e901c45d7d23","0fcc7d28eefa44c28986910d4944ac8c","77cf19957ddd4448926f1586a7446d4b","ef17b1592185419caa1503ae2943cbea","6c83ceb9a8b142b2936613a1b7897ee0","08e5ae26c9d1421ba7a9bddbb6227ff4","2986a626e1754459af687a5be88542b5","7265cdef1db54b3dbc8e4c067647c38e","14edcca58f004dd2a0ad2fb881703af3","0381aed6bf2849ef9c5c8ff6d69e3936","24f0d630067e485885e051e4fc98a526","b87021ebedd14b7489fd3dcbc84b8086","4b245e13921e45c6b060581be56f199b","5eed04ec4d8e40f2bbfacd1160e80087","3876b9b90e04415187e665f59bf4b306","c578c70c4a8c467b97a88ee92b4ab490","0ac2dbfbe38349d1964a01ff3a66b61d","92e1246330294bb49f20bd4dce981b80","272666dfe5cb407da211e13e6ed99905","c4600ec0bb634e2489b0981e85ec67ff","f457bd8fe28f4e3f9f9623edda8b2389","6e18b3bdd739439c91c60a608c22b55d","73fac8a349644ba5ba8901769825bc1f","261e654c5e1344f2bcc702567fbc9b53","190f58a662a94ee1ba2af4823951f513","f3af02af33b444bf94d1de1a87c3dc56","39089ac738d1446e9182d3c8c11ec61d","ce5dec7de7f946da864a38d31ab7f51f","56308bf59d674c75b387749be3ef5e8b","dafca9ab96534995956c061df62f426d","3416ace94dd24cbc88a63eabf15b333c","6d7b28e4021348b895c143ec2a13aca1","12b7e6ca945f414aaee2b06d594fe29b","49ccb679bc9f4f79830f47990d8b87c7","6a1433fc1c7a4e06bc73e744839dc862","c6e3732113f54d2db18d5ff9f6109276"]},"id":"qrKig1RVIVVx","outputId":"6cad013c-f22a-41cb-97ba-4161cc62b9bb","executionInfo":{"status":"ok","timestamp":1748092021232,"user_tz":-120,"elapsed":14575,"user":{"displayName":"Krittika Sardar","userId":"09484786934522619402"}}},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f67e31338b44c0f92c5b6d780cce52b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef17b1592185419caa1503ae2943cbea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3876b9b90e04415187e665f59bf4b306"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3af02af33b444bf94d1de1a87c3dc56"}},"metadata":{}}]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","class BERTDataset(Dataset):\n","    def __init__(self, texts, labels):\n","        self.input_ids, self.attn_mask, self.labels = encode(texts, labels)\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            'input_ids': self.input_ids[idx],\n","            'attention_mask': self.attn_mask[idx],\n","            'labels': self.labels[idx]\n","        }\n","\n","train_data = BERTDataset(train_df['text'].tolist(), train_df['label'].tolist())\n","test_data = BERTDataset(test_df['text'].tolist(), test_df['label'].tolist())\n","\n","train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n","test_loader = DataLoader(test_data, batch_size=16)\n"],"metadata":{"id":"LoYyraIQIZQL","executionInfo":{"status":"ok","timestamp":1748092199288,"user_tz":-120,"elapsed":153963,"user":{"displayName":"Krittika Sardar","userId":"09484786934522619402"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from transformers import DistilBertForSequenceClassification\n","\n","num_labels = len(set(train_df['label']))  # 3 for sentiment (0, 1, 2)\n","\n","model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n"],"metadata":{"id":"ew61y08FI1h8","colab":{"base_uri":"https://localhost:8080/","height":729,"referenced_widgets":["00ec53ac63d140f2a8bd8bf1a39f255b","f9ab8ef6a668477caf42f6b54636231a","a663468ded8448eaa2ded2a27111175e","27e3fab9a0c14439b63b729d93fe2de8","f4ff4284d22c461d9d3054bf996ccbd2","652e7fc16ef043e08eb8909068cd5ba4","7952d3cd51d24ef5afaad86bd5193492","18e9f8852b9f45a4918011592d289b81","81993383c5aa4473903c052ff6cbbb1a","858e655b716e456a8f1f3cd1e7b2f947","c41ee617ead84790bb4eb9521959dfdc"]},"outputId":"4b8fb3ef-6781-41ca-fb50-b7cdca6be144","executionInfo":{"status":"ok","timestamp":1748092213879,"user_tz":-120,"elapsed":8838,"user":{"displayName":"Krittika Sardar","userId":"09484786934522619402"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00ec53ac63d140f2a8bd8bf1a39f255b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["DistilBertForSequenceClassification(\n","  (distilbert): DistilBertModel(\n","    (embeddings): Embeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (transformer): Transformer(\n","      (layer): ModuleList(\n","        (0-5): 6 x TransformerBlock(\n","          (attention): DistilBertSdpaAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n","  (classifier): Linear(in_features=768, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n",")"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["import torch.nn as nn\n","from torch.optim import AdamW\n","from tqdm import tqdm\n","import os\n","\n","# Set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","# Optimizer and loss\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","criterion = nn.CrossEntropyLoss()\n","\n","# Config\n","EPOCHS = 10\n","CHECKPOINT_DIR = \"/content/drive/MyDrive/Web_Mining_DistilBert/checkpoints\"\n","os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n","latest_path = os.path.join(CHECKPOINT_DIR, \"latest.pt\")\n","best_path = os.path.join(CHECKPOINT_DIR, \"best.pt\")\n","\n","# Resume if latest checkpoint exists\n","start_epoch = 0\n","best_loss = float('inf')\n","\n","if os.path.exists(latest_path):\n","    print(\"Resuming from checkpoint...\")\n","    checkpoint = torch.load(latest_path, map_location=device)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    start_epoch = checkpoint['epoch']\n","    best_loss = checkpoint.get('best_loss', float('inf'))\n","    print(f\"Resumed from epoch {start_epoch} with best loss {best_loss:.4f}\")\n","\n","# Training loop\n","for epoch in range(start_epoch, EPOCHS):\n","    model.train()\n","    total_loss = 0\n","    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n","\n","    for batch in loop:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","        loss = criterion(outputs.logits, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        loop.set_postfix(loss=loss.item())\n","\n","    avg_loss = total_loss / len(train_loader)\n","    print(f\"Epoch {epoch+1} finished. Average Loss: {avg_loss:.4f}\")\n","\n","    # Save latest checkpoint\n","    torch.save({\n","        'epoch': epoch + 1,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'loss': avg_loss,\n","        'best_loss': best_loss\n","    }, latest_path)\n","    print(f\"ðŸ“¦ Latest checkpoint saved at {latest_path}\")\n","\n","    # Save best model if improved\n","    if avg_loss < best_loss:\n","        best_loss = avg_loss\n","        torch.save(model.state_dict(), best_path)\n","        print(f\"ðŸ† Best model updated and saved at {best_path} (Loss: {best_loss:.4f})\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZzHObklkJm7i","outputId":"74aa9599-15ad-4836-a630-93ec03c01261","executionInfo":{"status":"ok","timestamp":1748103219756,"user_tz":-120,"elapsed":10999291,"user":{"displayName":"Krittika Sardar","userId":"09484786934522619402"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6396/6396 [18:08<00:00,  5.88it/s, loss=0.344]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 finished. Average Loss: 0.5024\n","ðŸ“¦ Latest checkpoint saved at /content/drive/MyDrive/Web_Mining_DistilBert/checkpoints/latest.pt\n","ðŸ† Best model updated and saved at /content/drive/MyDrive/Web_Mining_DistilBert/checkpoints/best.pt (Loss: 0.5024)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6396/6396 [18:13<00:00,  5.85it/s, loss=0.071]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2 finished. Average Loss: 0.3510\n","ðŸ“¦ Latest checkpoint saved at /content/drive/MyDrive/Web_Mining_DistilBert/checkpoints/latest.pt\n","ðŸ† Best model updated and saved at /content/drive/MyDrive/Web_Mining_DistilBert/checkpoints/best.pt (Loss: 0.3510)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6396/6396 [18:13<00:00,  5.85it/s, loss=0.108]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3 finished. Average Loss: 0.2441\n","ðŸ“¦ Latest checkpoint saved at /content/drive/MyDrive/Web_Mining_DistilBert/checkpoints/latest.pt\n","ðŸ† Best model updated and saved at /content/drive/MyDrive/Web_Mining_DistilBert/checkpoints/best.pt (Loss: 0.2441)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6396/6396 [18:13<00:00,  5.85it/s, loss=0.72]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4 finished. Average Loss: 0.1613\n","ðŸ“¦ Latest checkpoint saved at /content/drive/MyDrive/Web_Mining_DistilBert/checkpoints/latest.pt\n","ðŸ† Best model updated and saved at /content/drive/MyDrive/Web_Mining_DistilBert/checkpoints/best.pt (Loss: 0.1613)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6396/6396 [18:13<00:00,  5.85it/s, loss=0.0443]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5 finished. Average Loss: 0.1102\n","ðŸ“¦ Latest checkpoint saved at /content/drive/MyDrive/Web_Mining_DistilBert/checkpoints/latest.pt\n","ðŸ† Best model updated and saved at /content/drive/MyDrive/Web_Mining_DistilBert/checkpoints/best.pt (Loss: 0.1102)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6396/6396 [18:14<00:00,  5.85it/s, loss=0.0228]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6 finished. Average Loss: 0.0803\n","ðŸ“¦ Latest checkpoint saved at /content/drive/MyDrive/Web_Mining_DistilBert/checkpoints/latest.pt\n","ðŸ† Best model updated and saved at /content/drive/MyDrive/Web_Mining_DistilBert/checkpoints/best.pt (Loss: 0.0803)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6396/6396 [18:13<00:00,  5.85it/s, loss=0.0102]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7 finished. Average Loss: 0.0635\n","ðŸ“¦ Latest checkpoint saved at /content/drive/MyDrive/Web_Mining_DistilBert/checkpoints/latest.pt\n","ðŸ† Best model updated and saved at /content/drive/MyDrive/Web_Mining_DistilBert/checkpoints/best.pt (Loss: 0.0635)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6396/6396 [18:13<00:00,  5.85it/s, loss=0.0177]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8 finished. Average Loss: 0.0529\n","ðŸ“¦ Latest checkpoint saved at /content/drive/MyDrive/Web_Mining_DistilBert/checkpoints/latest.pt\n","ðŸ† Best model updated and saved at /content/drive/MyDrive/Web_Mining_DistilBert/checkpoints/best.pt (Loss: 0.0529)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6396/6396 [18:13<00:00,  5.85it/s, loss=0.00122]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9 finished. Average Loss: 0.0442\n","ðŸ“¦ Latest checkpoint saved at /content/drive/MyDrive/Web_Mining_DistilBert/checkpoints/latest.pt\n","ðŸ† Best model updated and saved at /content/drive/MyDrive/Web_Mining_DistilBert/checkpoints/best.pt (Loss: 0.0442)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6396/6396 [18:13<00:00,  5.85it/s, loss=0.0177]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10 finished. Average Loss: 0.0388\n","ðŸ“¦ Latest checkpoint saved at /content/drive/MyDrive/Web_Mining_DistilBert/checkpoints/latest.pt\n","ðŸ† Best model updated and saved at /content/drive/MyDrive/Web_Mining_DistilBert/checkpoints/best.pt (Loss: 0.0388)\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, classification_report\n","\n","model.eval()\n","all_preds = []\n","all_labels = []\n","\n","with torch.no_grad():\n","    for batch in test_loader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","        preds = torch.argmax(outputs.logits, dim=1)\n","\n","        all_preds.extend(preds.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","print(\"Accuracy:\", accuracy_score(all_labels, all_preds))\n","print(classification_report(all_labels, all_preds))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jIHtxSYVK8QD","outputId":"4a31710b-d298-43b4-b6e1-c6e99d2e5277","executionInfo":{"status":"ok","timestamp":1748103522907,"user_tz":-120,"elapsed":88114,"user":{"displayName":"Krittika Sardar","userId":"09484786934522619402"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.8554117968963765\n","              precision    recall  f1-score   support\n","\n","           0       0.85      0.87      0.86      8527\n","           1       0.81      0.78      0.80      8528\n","           2       0.90      0.92      0.91      8528\n","\n","    accuracy                           0.86     25583\n","   macro avg       0.85      0.86      0.85     25583\n","weighted avg       0.85      0.86      0.85     25583\n","\n"]}]},{"cell_type":"code","source":["# Resume from a checkpoint\n","checkpoint_path = \"bert_epoch_2.pt\"  # replace with your file\n","\n","checkpoint = torch.load(checkpoint_path)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","start_epoch = checkpoint['epoch']\n"],"metadata":{"id":"CnBGJlSYLD8x"},"execution_count":null,"outputs":[]}]}
